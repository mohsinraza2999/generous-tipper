experiment_name: generous_tipper_v1
#'xgboost',
seed: 42
scoring:
  accuracy: accuracy
  precision: precision
  recall: recall
  f1: f1
target_matrix: f1
cv: 5
n_jobs: -1
model:
  type: ['random_forest', 'logistic_regression']
xgboost:
  params:
    model__n_estimators: [100, 300]
    model__learning_rate: [0.03, 0.04]
    model__max_depth: [6, 8]
    model__min_child_weight: [1, 3]

random_forest:
  params:
    model__n_estimators: [100]
    model__min_samples_split: [4]
    model__min_samples_leaf: [3]
    model__max_samples: [0.6]
    model__max_features: [0.8]
    model__max_depth: [null]

logistic_regression:
  params:
    model__C: [0.1, 1.0]
    model__penalty: ["l2"]
    model__solver: ["lbfgs"]
model_dir: artifacts
training:
  batch_size: 64
  epochs: 10
  validation_split: 0.2
data:
  processed_path: data/processed/
  name: processed_data.csv
  seed: 42
  test_size: 0.2
  target: generous

logging:
  mlflow: true